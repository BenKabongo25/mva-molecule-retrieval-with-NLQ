{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b7189-d0c9-4b6c-adc5-abe77f81e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031e660c-50c4-4902-9ee2-ef43084cafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "\n",
    "\n",
    "from alignment import AlignmentModel,Discriminator, gradient_penalty\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a90423-f5a2-4703-9875-bbdb2fedbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_triplet_loss(graph_embeddings, text_embeddings, margin = 0.3):\n",
    "    cosine = pairwise_cosine_similarity(text_embeddings, graph_embeddings) # compute cosine similarity between each pairs (texts, graphs)\n",
    "    positive_sample = cosine.diag() # get similarity between anchor and positive sample where anchor could be the text representation and positive sample the graph represention and vice versa\n",
    "    cosine = cosine.fill_diagonal_(-2) # set diag val to a minimum possible value of similarity to get hard negetive example by argmax\n",
    "    loss = torch.clamp(torch.max(cosine, axis = 1)[0] - positive_sample + margin,0)\n",
    "    loss += torch.clamp(torch.max(cosine, axis = 0)[0] - positive_sample +  margin,0)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887366c-c103-4118-9e03-b5572ecb9e69",
   "metadata": {},
   "source": [
    "### **LOADING DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a2e6c5-717c-492a-b6af-9480f3264312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer)\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59adca-8977-4f84-81cf-f07e58c6263e",
   "metadata": {},
   "source": [
    "### **LOADING FINETUNED MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4161d710-8bc9-4006-a742-dd4117ee1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlignmentModel(\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (text_forward): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv1): AntiSymmetricConv(300, phi=GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )), num_iters=1, epsilon=0.1, gamma=0.1)\n",
       "  (conv2): AntiSymmetricConv(300, phi=GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )), num_iters=1, epsilon=0.1, gamma=0.1)\n",
       "  (conv3): AntiSymmetricConv(300, phi=GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )), num_iters=1, epsilon=0.1, gamma=0.1)\n",
       "  (graph_forward): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (graph_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformers = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type ='TransformerConv' )\n",
    "model_transformers.to(device)\n",
    "\n",
    "path_to_model_transformer = 'TransformerConv_model_pretrained_text.pt'\n",
    "checkpoint = torch.load(path_to_model_transformer)\n",
    "model_transformers.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_transformers.eval()\n",
    "\n",
    "model_GPS = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type = 'GPS')\n",
    "model_GPS.to(device)\n",
    "\n",
    "path_to_model_gps = 'GPS_model_pretrained_text.pt'\n",
    "checkpoint = torch.load(path_to_model_gps)\n",
    "model_GPS.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_GPS.eval()\n",
    "\n",
    "model_GIN = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type = 'GIN')\n",
    "model_GIN .to(device)\n",
    "\n",
    "path_to_model_gin= 'GIN_model_pretrained_text.pt'\n",
    "checkpoint = torch.load(path_to_model_gin)\n",
    "model_GIN .load_state_dict(checkpoint['model_state_dict'])\n",
    "model_GIN .eval()\n",
    "\n",
    "model_EGC = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type = 'EGC')\n",
    "model_EGC .to(device)\n",
    "\n",
    "path_to_model_egc= 'EGC_model_pretrained_text.pt'\n",
    "checkpoint = torch.load(path_to_model_egc)\n",
    "model_EGC .load_state_dict(checkpoint['model_state_dict'])\n",
    "model_EGC .eval()\n",
    "\n",
    "model_Antisymmetric = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type = 'Antisymmetric')\n",
    "model_Antisymmetric .to(device)\n",
    "\n",
    "path_to_model_Antisymmetric= 'Antisymmetric_model_pretrained_text.pt'\n",
    "checkpoint = torch.load(path_to_model_Antisymmetric)\n",
    "model_Antisymmetric .load_state_dict(checkpoint['model_state_dict'])\n",
    "model_Antisymmetric .eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688cb41-59ac-41b7-b190-3444ce003653",
   "metadata": {},
   "source": [
    "### **PREDICTIONS ON THE VALIDATION TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dae8177-a0dd-43fa-b306-3787bcc0e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/104 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "100%|██████████| 104/104 [00:36<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_transformers = []\n",
    "text_transformers = [] \n",
    "\n",
    "graph_gps = []\n",
    "text_gps = []\n",
    "\n",
    "graph_gin = []\n",
    "text_gin = []\n",
    "\n",
    "graph_egc = []\n",
    "text_egc = []\n",
    "\n",
    "graph_Antisymmetric = []\n",
    "text_Antisymmetric = []\n",
    "\n",
    "for batch in tqdm(val_loader):\n",
    "    output = model_transformers.forward_graph(batch.to(device))\n",
    "    graph_transformers.extend(output.tolist())\n",
    "        \n",
    "    output = model_GPS.forward_graph(batch.to(device))\n",
    "    graph_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_GIN.forward_graph(batch.to(device))\n",
    "    graph_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_EGC.forward_graph(batch.to(device))\n",
    "    graph_egc.extend(output.tolist())\n",
    "\n",
    "    output = model_Antisymmetric.forward_graph(batch.to(device))\n",
    "    graph_Antisymmetric.extend(output.tolist())\n",
    "    \n",
    "#for batch in val_loader:\n",
    "    output = model_transformers.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_transformers.extend(output.tolist())\n",
    "        \n",
    "    output = model_GPS.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_gps.extend(output.tolist())\n",
    "    \n",
    "    output = model_GIN.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_EGC.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_egc.extend(output.tolist())\n",
    "\n",
    "    output = model_Antisymmetric.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_Antisymmetric.extend(output.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc14a04-1688-4222-9798-b1a5c010519c",
   "metadata": {},
   "source": [
    "### **EVALUATION ON THE VALIDATION TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a742ca49-069c-4a51-bc52-310c8986ed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8343528594524177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_transformers = cosine_similarity(text_transformers, graph_transformers)\n",
    "y_true = np.eye(len(similarity_transformers))\n",
    "score_transformers = label_ranking_average_precision_score(y_true, similarity_transformers)\n",
    "score_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c01fd3f-177e-4534-82c4-129f11375b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8533235998279229"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_gps = cosine_similarity(text_gps, graph_gps)\n",
    "y_true = np.eye(len(similarity_gps))\n",
    "score_gps = label_ranking_average_precision_score(y_true, similarity_gps)\n",
    "score_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52624d0-586f-4801-915b-c34c6e6be308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189375551353847"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_gin = cosine_similarity(text_gin, graph_gin)\n",
    "y_true = np.eye(len(similarity_gin))\n",
    "score_gin = label_ranking_average_precision_score(y_true, similarity_gin)\n",
    "score_gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c526d2-9cc7-4dd4-a7cb-ebc74b9f311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8352084282545114"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_egc = cosine_similarity(text_egc, graph_egc)\n",
    "y_true = np.eye(len(similarity_egc))\n",
    "score_egc = label_ranking_average_precision_score(y_true, similarity_egc)\n",
    "score_egc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17923ddf-711b-49c8-b0c5-b798e1cc8ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8438302913068332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_Antisymmetric = cosine_similarity(text_Antisymmetric, graph_Antisymmetric)\n",
    "y_true = np.eye(len(similarity_Antisymmetric))\n",
    "score_Antisymmetric = label_ranking_average_precision_score(y_true, similarity_Antisymmetric)\n",
    "score_Antisymmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c3c91f-c83d-4f76-85d8-182dd570b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921195846260427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ranking_average_precision_score(y_true, similarity_gps + similarity_transformers + similarity_egc + similarity_gin + similarity_Antisymmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac60d96-a6ad-4649-a9cc-2378b52968b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de9e38-53be-45f6-8dec-dbd95d8fb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in np.arange(0.2,0.5,1/30):\n",
    "    for beta in np.arange(0.2,0.5,1/30):\n",
    "        print(f'{alpha:.2f}, {beta:.2f} : {label_ranking_average_precision_score(y_true, alpha*similarity_gps + beta*similarity_transformers + (1-alpha-beta)*similarity_gin)*100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5296851-15b1-48bb-90c9-664b3a1f8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(similarity_transformers).to_csv('similarity_transformers_val.csv', index=False)\n",
    "pd.DataFrame(similarity_gps).to_csv('similarity_gps_val.csv', index=False)\n",
    "pd.DataFrame(similarity_gin).to_csv('similarity_gin_val.csv', index=False)\n",
    "pd.DataFrame(similarity_egc).to_csv('similarity_egc_val.csv', index=False)\n",
    "pd.DataFrame(similarity_Antisymmetric).to_csv('similarity_antisymmetric_val.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52032c-4987-4287-8b0b-baa13b5df740",
   "metadata": {},
   "source": [
    "### **PREDICTIONS ON TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3748e7c-b45d-44bd-9a1b-7bd1524f0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids')\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "graph_transformers = []\n",
    "text_transformers = [] \n",
    "\n",
    "graph_gps = []\n",
    "text_gps = []\n",
    "\n",
    "graph_gin = []\n",
    "text_gin = []\n",
    "\n",
    "graph_egc = []\n",
    "text_egc = []\n",
    "\n",
    "graph_Antisymmetric = []\n",
    "text_Antisymmetric = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c58256-41df-410f-a51e-0bdb2a6b87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "test_graph_loader = DataLoader(test_cids_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in test_graph_loader:\n",
    "    output = model_transformers.forward_graph(batch.to(device))\n",
    "    graph_transformers.extend(output.tolist())\n",
    "        \n",
    "    output = model_GPS.forward_graph(batch.to(device))\n",
    "    graph_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_GIN.forward_graph(batch.to(device))\n",
    "    graph_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_EGC.forward_graph(batch.to(device))\n",
    "    graph_egc.extend(output.tolist())\n",
    "\n",
    "    output = model_Antisymmetric.forward_graph(batch.to(device))\n",
    "    graph_Antisymmetric.extend(output.tolist())\n",
    "    \n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in test_text_loader:\n",
    "    output = model_transformers.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_transformers.extend(output.tolist())\n",
    "        \n",
    "    output = model_GPS.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_GIN.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_EGC.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_egc.extend(output.tolist())\n",
    "\n",
    "    output = model_Antisymmetric.forward_text(batch['input_ids'].to(device),batch['attention_mask'].to(device))\n",
    "    text_Antisymmetric.extend(output.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239282a9-3766-4324-9896-5fa74bdb9072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "360c1e4c-221e-4b31-81fc-8e0caffbc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_transformers = cosine_similarity(text_transformers,  graph_transformers)\n",
    "similarity_gps = cosine_similarity(text_gps, graph_gps)\n",
    "similarity_gin = cosine_similarity(text_gin, graph_gin)\n",
    "similarity_egc = cosine_similarity(text_egc, graph_egc)\n",
    "similarity_antisymmetric = cosine_similarity(text_Antisymmetric, graph_Antisymmetric)\n",
    "\n",
    "\n",
    "similarity = similarity_transformers + similarity_gps + similarity_gin + similarity_antisymmetric + similarity_egc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a0616b5-37b6-4f65-9155-1940f2ae6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(similarity_transformers).to_csv('similarity_transformers_test.csv', index=False)\n",
    "pd.DataFrame(similarity_gps).to_csv('similarity_gps_test.csv', index=False)\n",
    "pd.DataFrame(similarity_gin).to_csv('similarity_gin_test.csv', index=False)\n",
    "pd.DataFrame(similarity_egc).to_csv('similarity_egc_test.csv', index=False)\n",
    "pd.DataFrame(similarity_Antisymmetric).to_csv('similarity_antisymmetric_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e609ee-ff59-4d77-9719-e97fd84a21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('submission_gin+gps+egc+antisymmetric+transformerconv_text_pretrained_08921.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca85e24-1fff-41a8-9baa-f260f6e5067a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bdf53-08d4-4bc4-8cc9-9c353de8a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_transformers = np.argsort(-similarity_transformers, axis = 1) + 1\n",
    "order_gps = np.argsort(-similarity_gps, axis = 1) + 1\n",
    "order_coeff = 2/(order_transformers + order_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddba23-b98f-4645-a5cc-1c43ef18cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.sign(similarity_transformers)*np.log(1 + 1/order_transformers)*np.abs(similarity_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4ab5b-1e0b-4554-8930-2d1b0e66b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ranking_average_precision_score(y_true, coefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
