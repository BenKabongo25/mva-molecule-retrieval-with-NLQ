{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b7189-d0c9-4b6c-adc5-abe77f81e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031e660c-50c4-4902-9ee2-ef43084cafb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32334/3341702528.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "\n",
    "\n",
    "from alignment import AlignmentModel,Discriminator, gradient_penalty, CombinedModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887366c-c103-4118-9e03-b5572ecb9e69",
   "metadata": {},
   "source": [
    "### **LOADING DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a2e6c5-717c-492a-b6af-9480f3264312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer)\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59adca-8977-4f84-81cf-f07e58c6263e",
   "metadata": {},
   "source": [
    "### **LOADING FINETUNED MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ab0ab8-5834-4b66-813f-be3ca3e2e8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlignmentModel(\n",
       "  (text_encoder): TextEncoder(\n",
       "    (text_encoder): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (text_forward): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "    (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (graph_encoder): GraphEncoder(\n",
       "    (gnns): ModuleList(\n",
       "      (0-4): 5 x TransformerConv(768, 768, heads=6)\n",
       "    )\n",
       "    (projection): Linear(in_features=300, out_features=768, bias=True)\n",
       "    (graph_norms): ModuleList(\n",
       "      (0-4): 5 x LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (graph_forward): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_model= 'supergat_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_supergat = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'SuperGat', n_layers = 5)\n",
    "model_supergat.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_supergat.eval()\n",
    "model_supergat.to(device)\n",
    "\n",
    "\n",
    "path_to_model= 'anti_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_anti = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'Antisymmetric', n_layers = 5)\n",
    "model_anti.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_anti.eval()\n",
    "model_anti.to(device)\n",
    "\n",
    "\n",
    "path_to_model= 'gps_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_gps = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'GPS', n_layers = 5)\n",
    "model_gps.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_gps.eval()\n",
    "model_gps.to(device)\n",
    "\n",
    "\n",
    "path_to_model= 'transformer_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_trans = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'TransformerConv', n_layers = 5)\n",
    "model_trans.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_trans.eval()\n",
    "model_trans.to(device)\n",
    "\n",
    "path_to_model= 'gat_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_gat = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'GATv2Conv', n_layers = 5)\n",
    "model_gat.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_gat.eval()\n",
    "model_gat.to(device)\n",
    "\n",
    "path_to_model= 'gin_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_gin = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'GIN', n_layers = 5)\n",
    "model_gin.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_gin.eval()\n",
    "model_gin.to(device)\n",
    "\n",
    "\n",
    "path_to_model= 'dir_supergat_efficient.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_dir_supergat = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'DirGNNConv_supergat', n_layers = 5)\n",
    "model_dir_supergat.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_dir_supergat.eval()\n",
    "model_dir_supergat.to(device)\n",
    "\n",
    "\n",
    "path_to_model= 'gat_kv_plm.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_gat_kv_plm = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'GATv2Conv', n_layers = 5)\n",
    "model_gat_kv_plm.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_gat_kv_plm.eval()\n",
    "model_gat_kv_plm.to(device)\n",
    "\n",
    "path_to_model= 'transformer_kv_plm.pt'\n",
    "checkpoint = torch.load(path_to_model)\n",
    "model_trans_kv_plm = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'TransformerConv', n_layers = 5)\n",
    "model_trans_kv_plm.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_trans_kv_plm.eval()\n",
    "model_trans_kv_plm.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572622b-a11d-4413-9116-d5323becb44b",
   "metadata": {},
   "source": [
    "### **PREDICTIONS ON THE VALIDATION TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fc27a4-dedc-4838-9c5e-a8a3ce179dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [01:21<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_anti = []\n",
    "text_anti = [] \n",
    "\n",
    "graph_supergat = []\n",
    "text_supergat = [] \n",
    "\n",
    "graph_gps = []\n",
    "text_gps = [] \n",
    "\n",
    "graph_trans = []\n",
    "text_trans = []\n",
    "\n",
    "graph_gat = []\n",
    "text_gat = []\n",
    "\n",
    "graph_gin = []\n",
    "text_gin = []\n",
    "\n",
    "graph_dir_supergat = []\n",
    "text_dir_supergat = [] \n",
    "\n",
    "graph_trans_kv_plm = []\n",
    "text_trans_kv_plm = []\n",
    "\n",
    "graph_gat_kv_plm = []\n",
    "text_gat_kv_plm = []\n",
    "\n",
    "for batch in tqdm(val_loader):\n",
    "    output = model_anti.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_anti.extend(output.tolist())\n",
    "\n",
    "    output = model_supergat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_gps.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_trans.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_trans.extend(output.tolist())\n",
    "\n",
    "    output = model_gat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gat.extend(output.tolist())\n",
    "\n",
    "    output = model_gin.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_dir_supergat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_dir_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_trans_kv_plm.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_trans_kv_plm.extend(output.tolist())\n",
    "\n",
    "    output = model_gat_kv_plm.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gat_kv_plm.extend(output.tolist())\n",
    "\n",
    "    \n",
    "#for batch in val_loader:\n",
    "\n",
    "    output = model_anti.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_anti.extend(output.tolist())\n",
    "\n",
    "    output = model_supergat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_gps.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_trans.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_trans.extend(output.tolist())\n",
    "\n",
    "    output = model_gat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gat.extend(output.tolist())\n",
    "\n",
    "    output = model_gin.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_dir_supergat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_dir_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_trans_kv_plm.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_trans_kv_plm.extend(output.tolist())\n",
    "\n",
    "    output = model_gat_kv_plm.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gat_kv_plm.extend(output.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2319cf-4225-44c3-af2c-974df76c1244",
   "metadata": {},
   "source": [
    "### **EVALUATION ON THE VALIDATION TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c94145-4eb7-45d0-b96f-df0640231f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_anti = cosine_similarity(text_anti, graph_anti)\n",
    "similarity_supergat = cosine_similarity(text_supergat, graph_supergat)\n",
    "similarity_gps = cosine_similarity(text_gps, graph_gps)\n",
    "similarity_trans = cosine_similarity(text_trans, graph_trans)\n",
    "similarity_gat = cosine_similarity(text_gat, graph_gat)\n",
    "similarity_gin = cosine_similarity(text_gin, graph_gin)\n",
    "similarity_dir_supergat = cosine_similarity(text_dir_supergat, graph_dir_supergat)\n",
    "similarity_trans_kv_plm = cosine_similarity(text_trans_kv_plm , graph_trans_kv_plm )\n",
    "similarity_gat_kv_plm = cosine_similarity(text_gat_kv_plm , graph_gat_kv_plm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e63071-a08a-41e5-af92-24015e9e30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity =  similarity_trans + similarity_gat + (similarity_supergat + similarity_anti + similarity_gps + similarity_gin + similarity_dir_supergat) /2 + (similarity_trans_kv_plm + similarity_gat_kv_plm)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65c63a07-e69d-40de-b6bd-a06ce2ad4bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9380643621440066"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.eye(len(similarity))\n",
    "label_ranking_average_precision_score(y_true, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52032c-4987-4287-8b0b-baa13b5df740",
   "metadata": {},
   "source": [
    "### **PREDICTIONS ON TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3748e7c-b45d-44bd-9a1b-7bd1524f0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids')\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81bf622d-b89a-4845-ad8f-90425d88f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "100%|██████████| 104/104 [00:10<00:00, 10.23it/s]\n",
      "100%|██████████| 104/104 [01:13<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_anti = []\n",
    "text_anti = [] \n",
    "\n",
    "graph_supergat = []\n",
    "text_supergat = [] \n",
    "\n",
    "graph_gps = []\n",
    "text_gps = [] \n",
    "\n",
    "graph_trans = []\n",
    "text_trans = []\n",
    "\n",
    "graph_gat = []\n",
    "text_gat = []\n",
    "\n",
    "graph_gin = []\n",
    "text_gin = []\n",
    "\n",
    "graph_dir_supergat = []\n",
    "text_dir_supergat = [] \n",
    "\n",
    "graph_trans_kv_plm = []\n",
    "text_trans_kv_plm = []\n",
    "\n",
    "graph_gat_kv_plm = []\n",
    "text_gat_kv_plm = []\n",
    "\n",
    "test_graph_loader = DataLoader(test_cids_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in tqdm(test_graph_loader):\n",
    "    output = model_anti.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_anti.extend(output.tolist())\n",
    "\n",
    "    output = model_supergat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_gps.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_trans.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_trans.extend(output.tolist())\n",
    "\n",
    "    output = model_gat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gat.extend(output.tolist())\n",
    "\n",
    "    output = model_gin.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_dir_supergat.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_dir_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_trans_kv_plm.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_trans_kv_plm.extend(output.tolist())\n",
    "\n",
    "    output = model_gat_kv_plm.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "    graph_gat_kv_plm.extend(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in tqdm(test_text_loader):\n",
    "\n",
    "    output = model_anti.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_anti.extend(output.tolist())\n",
    "\n",
    "    output = model_supergat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_gps.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gps.extend(output.tolist())\n",
    "\n",
    "    output = model_trans.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_trans.extend(output.tolist())\n",
    "\n",
    "    output = model_gat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gat.extend(output.tolist())\n",
    "\n",
    "    output = model_gin.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gin.extend(output.tolist())\n",
    "\n",
    "    output = model_dir_supergat.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_dir_supergat.extend(output.tolist())\n",
    "\n",
    "    output = model_trans_kv_plm.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_trans_kv_plm.extend(output.tolist())\n",
    "\n",
    "    output = model_gat_kv_plm.forward_text(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    text_gat_kv_plm.extend(output.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89c3e3aa-b3b8-448b-be8b-3249335a466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_anti = cosine_similarity(text_anti, graph_anti)\n",
    "similarity_supergat = cosine_similarity(text_supergat, graph_supergat)\n",
    "similarity_gps = cosine_similarity(text_gps, graph_gps)\n",
    "similarity_trans = cosine_similarity(text_trans, graph_trans)\n",
    "similarity_gat = cosine_similarity(text_gat, graph_gat)\n",
    "similarity_gin = cosine_similarity(text_gin, graph_gin)\n",
    "similarity_dir_supergat = cosine_similarity(text_dir_supergat, graph_dir_supergat)\n",
    "similarity_trans_kv_plm = cosine_similarity(text_trans_kv_plm , graph_trans_kv_plm )\n",
    "similarity_gat_kv_plm = cosine_similarity(text_gat_kv_plm , graph_gat_kv_plm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7baa5dc-e838-4ed6-a8e4-74ef5c147dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity =  similarity_trans + (similarity_supergat + similarity_anti + similarity_gps + similarity_gin + similarity_dir_supergat) /2 + similarity_gat + (similarity_trans_kv_plm + similarity_gat_kv_plm)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38591553-b1e3-4a20-9326-73cd2bee69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('submission_combined_efficient_kv_plm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
