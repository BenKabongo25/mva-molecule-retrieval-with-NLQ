{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "\n",
    "\n",
    "from alignment import AlignmentModel,Discriminator, gradient_penalty\n",
    "from moemodel import MOEModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_triplet_loss(graph_embeddings, text_embeddings, margin = 0.3):\n",
    "    cosine = pairwise_cosine_similarity(text_embeddings, graph_embeddings) # compute cosine similarity between each pairs (texts, graphs)\n",
    "    positive_sample = cosine.diag() # get similarity between anchor and positive sample where anchor could be the text representation and positive sample the graph represention and vice versa\n",
    "    cosine = cosine.fill_diagonal_(-2) # set diag val to a minimum possible value of similarity to get hard negetive example by argmax\n",
    "    loss = torch.clamp(torch.max(cosine, axis = 1)[0] - positive_sample + margin,0)\n",
    "    loss += torch.clamp(torch.max(cosine, axis = 0)[0] - positive_sample +  margin,0)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer)\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlignmentModel(in_channels=300, out_channels=300, graph_attention_head=6, type = 'Antisymmetric')\n",
    "# expert_type = ['GPS', 'EGC', 'TransformerConv']\n",
    "\n",
    "# model = MOEModel(300, 300, 6, expert_type)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "model.to(device)\n",
    "\n",
    "discriminator=Discriminator(300,300)\n",
    "optimizer_discriminator = optim.AdamW(discriminator.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "discriminator.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = torch.load('model8.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "loss = 0\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000\n",
    "best_validation_mrr = 0\n",
    "discriminator_iteration = 3\n",
    "lambda_gp = 1\n",
    "gamma = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH1-----\n",
      "Iteration: 24800, Time: 17727.4163 s, training loss: 0.0265\n",
      "Iteration: 24850, Time: 17758.5371 s, training loss: 0.0309\n",
      "Iteration: 24900, Time: 17789.7504 s, training loss: 0.0312\n",
      "Iteration: 24950, Time: 17821.1303 s, training loss: 0.0241\n",
      "Iteration: 25000, Time: 17852.0777 s, training loss: 0.0325\n",
      "Iteration: 25050, Time: 17883.0199 s, training loss: 0.0314\n",
      "Iteration: 25100, Time: 17914.1380 s, training loss: 0.0270\n",
      "Iteration: 25150, Time: 17945.1350 s, training loss: 0.0357\n",
      "Iteration: 25200, Time: 17976.3632 s, training loss: 0.0278\n",
      "Iteration: 25250, Time: 18007.4518 s, training loss: 0.0311\n",
      "Iteration: 25300, Time: 18038.1156 s, training loss: 0.0310\n",
      "Iteration: 25350, Time: 18068.3183 s, training loss: 0.0332\n",
      "Iteration: 25400, Time: 18098.5687 s, training loss: 0.0332\n",
      "Iteration: 25450, Time: 18128.9218 s, training loss: 0.0300\n",
      "Iteration: 25500, Time: 18159.1207 s, training loss: 0.0331\n",
      "Iteration: 25550, Time: 18189.4228 s, training loss: 0.0364\n",
      "Iteration: 25600, Time: 18219.6745 s, training loss: 0.0292\n",
      "-----EPOCH1----- done.  Validation loss:  0.04396440203373249\n",
      "-----EPOCH1----- done.  Validation MRR:  0.8404316569356196\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_discriminator0.pt\n",
      "-----EPOCH2-----\n",
      "Iteration: 25650, Time: 18270.2231 s, training loss: 0.0290\n",
      "Iteration: 25700, Time: 18301.0491 s, training loss: 0.0319\n",
      "Iteration: 25750, Time: 18332.1357 s, training loss: 0.0289\n",
      "Iteration: 25800, Time: 18363.2828 s, training loss: 0.0295\n",
      "Iteration: 25850, Time: 18394.2216 s, training loss: 0.0299\n",
      "Iteration: 25900, Time: 18425.3353 s, training loss: 0.0330\n",
      "Iteration: 25950, Time: 18456.7276 s, training loss: 0.0346\n",
      "Iteration: 26000, Time: 18487.7344 s, training loss: 0.0358\n",
      "Iteration: 26050, Time: 18518.7954 s, training loss: 0.0334\n",
      "Iteration: 26100, Time: 18549.9235 s, training loss: 0.0249\n",
      "Iteration: 26150, Time: 18581.0510 s, training loss: 0.0349\n",
      "Iteration: 26200, Time: 18612.1502 s, training loss: 0.0281\n",
      "Iteration: 26250, Time: 18643.2594 s, training loss: 0.0314\n",
      "Iteration: 26300, Time: 18674.4201 s, training loss: 0.0322\n",
      "Iteration: 26350, Time: 18705.6371 s, training loss: 0.0287\n",
      "Iteration: 26400, Time: 18736.5187 s, training loss: 0.0274\n",
      "-----EPOCH2----- done.  Validation loss:  0.04509116576697964\n",
      "-----EPOCH2----- done.  Validation MRR:  0.8400440361527909\n",
      "-----EPOCH3-----\n",
      "Iteration: 26450, Time: 18781.2858 s, training loss: 0.0272\n",
      "Iteration: 26500, Time: 18812.0193 s, training loss: 0.0357\n",
      "Iteration: 26550, Time: 18843.1396 s, training loss: 0.0325\n",
      "Iteration: 26600, Time: 18874.0497 s, training loss: 0.0304\n",
      "Iteration: 26650, Time: 18904.9608 s, training loss: 0.0263\n",
      "Iteration: 26700, Time: 18936.1172 s, training loss: 0.0311\n",
      "Iteration: 26750, Time: 18967.3514 s, training loss: 0.0279\n",
      "Iteration: 26800, Time: 18997.9127 s, training loss: 0.0312\n",
      "Iteration: 26850, Time: 19028.1450 s, training loss: 0.0306\n",
      "Iteration: 26900, Time: 19058.4328 s, training loss: 0.0304\n",
      "Iteration: 26950, Time: 19088.8231 s, training loss: 0.0296\n",
      "Iteration: 27000, Time: 19119.2350 s, training loss: 0.0325\n",
      "Iteration: 27050, Time: 19149.8358 s, training loss: 0.0269\n",
      "Iteration: 27100, Time: 19180.3023 s, training loss: 0.0315\n",
      "Iteration: 27150, Time: 19211.2706 s, training loss: 0.0350\n",
      "Iteration: 27200, Time: 19242.1568 s, training loss: 0.0321\n",
      "Iteration: 27250, Time: 19272.9401 s, training loss: 0.0297\n",
      "-----EPOCH3----- done.  Validation loss:  0.04144468276689832\n",
      "-----EPOCH3----- done.  Validation MRR:  0.8421380797943755\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_discriminator2.pt\n",
      "-----EPOCH4-----\n",
      "Iteration: 27300, Time: 19322.1174 s, training loss: 0.0363\n",
      "Iteration: 27350, Time: 19352.4247 s, training loss: 0.0260\n",
      "Iteration: 27400, Time: 19382.6185 s, training loss: 0.0313\n",
      "Iteration: 27450, Time: 19412.7038 s, training loss: 0.0286\n",
      "Iteration: 27500, Time: 19443.1372 s, training loss: 0.0289\n",
      "Iteration: 27550, Time: 19473.3176 s, training loss: 0.0266\n",
      "Iteration: 27600, Time: 19503.3214 s, training loss: 0.0347\n",
      "Iteration: 27650, Time: 19533.5162 s, training loss: 0.0269\n",
      "Iteration: 27700, Time: 19563.8349 s, training loss: 0.0274\n",
      "Iteration: 27750, Time: 19594.1193 s, training loss: 0.0306\n",
      "Iteration: 27800, Time: 19624.4239 s, training loss: 0.0339\n",
      "Iteration: 27850, Time: 19654.5011 s, training loss: 0.0343\n",
      "Iteration: 27900, Time: 19684.8211 s, training loss: 0.0336\n",
      "Iteration: 27950, Time: 19715.0047 s, training loss: 0.0278\n",
      "Iteration: 28000, Time: 19745.3208 s, training loss: 0.0318\n",
      "Iteration: 28050, Time: 19775.5203 s, training loss: 0.0312\n",
      "-----EPOCH4----- done.  Validation loss:  0.04134120402606921\n",
      "-----EPOCH4----- done.  Validation MRR:  0.8404103171679497\n",
      "-----EPOCH5-----\n",
      "Iteration: 28100, Time: 19820.5245 s, training loss: 0.0340\n",
      "Iteration: 28150, Time: 19851.1188 s, training loss: 0.0302\n",
      "Iteration: 28200, Time: 19881.5305 s, training loss: 0.0283\n",
      "Iteration: 28250, Time: 19911.7359 s, training loss: 0.0282\n",
      "Iteration: 28300, Time: 19941.9328 s, training loss: 0.0348\n",
      "Iteration: 28350, Time: 19972.3537 s, training loss: 0.0277\n",
      "Iteration: 28400, Time: 20002.6639 s, training loss: 0.0290\n",
      "Iteration: 28450, Time: 20032.9200 s, training loss: 0.0326\n",
      "Iteration: 28500, Time: 20063.3491 s, training loss: 0.0341\n",
      "Iteration: 28550, Time: 20093.6153 s, training loss: 0.0334\n",
      "Iteration: 28600, Time: 20124.1773 s, training loss: 0.0275\n",
      "Iteration: 28650, Time: 20154.5476 s, training loss: 0.0318\n",
      "Iteration: 28700, Time: 20184.9158 s, training loss: 0.0327\n",
      "Iteration: 28750, Time: 20215.4461 s, training loss: 0.0300\n",
      "Iteration: 28800, Time: 20246.3201 s, training loss: 0.0290\n",
      "Iteration: 28850, Time: 20277.2367 s, training loss: 0.0338\n",
      "Iteration: 28900, Time: 20308.9362 s, training loss: 0.0287\n",
      "-----EPOCH5----- done.  Validation loss:  0.04785361200965081\n",
      "-----EPOCH5----- done.  Validation MRR:  0.8397970282587708\n",
      "-----EPOCH6-----\n",
      "Iteration: 28950, Time: 20354.2221 s, training loss: 0.0325\n",
      "Iteration: 29000, Time: 20385.3631 s, training loss: 0.0287\n",
      "Iteration: 29050, Time: 20416.4592 s, training loss: 0.0344\n",
      "Iteration: 29100, Time: 20447.3684 s, training loss: 0.0325\n",
      "Iteration: 29150, Time: 20478.3200 s, training loss: 0.0306\n",
      "Iteration: 29200, Time: 20509.3518 s, training loss: 0.0318\n",
      "Iteration: 29250, Time: 20540.7207 s, training loss: 0.0298\n",
      "Iteration: 29300, Time: 20572.2546 s, training loss: 0.0269\n",
      "Iteration: 29350, Time: 20603.6282 s, training loss: 0.0328\n",
      "Iteration: 29400, Time: 20634.7596 s, training loss: 0.0264\n",
      "Iteration: 29450, Time: 20665.9531 s, training loss: 0.0291\n",
      "Iteration: 29500, Time: 20696.8840 s, training loss: 0.0287\n",
      "Iteration: 29550, Time: 20727.7137 s, training loss: 0.0313\n",
      "Iteration: 29600, Time: 20758.8214 s, training loss: 0.0298\n",
      "Iteration: 29650, Time: 20790.0192 s, training loss: 0.0309\n",
      "Iteration: 29700, Time: 20820.9189 s, training loss: 0.0311\n",
      "-----EPOCH6----- done.  Validation loss:  0.042083324130194694\n",
      "-----EPOCH6----- done.  Validation MRR:  0.8412342353638567\n",
      "-----EPOCH7-----\n",
      "Iteration: 29750, Time: 20865.9216 s, training loss: 0.0219\n",
      "Iteration: 29800, Time: 20896.3359 s, training loss: 0.0329\n",
      "Iteration: 29850, Time: 20926.8238 s, training loss: 0.0307\n",
      "Iteration: 29900, Time: 20957.3192 s, training loss: 0.0309\n",
      "Iteration: 29950, Time: 20987.6364 s, training loss: 0.0298\n",
      "Iteration: 30000, Time: 21018.1718 s, training loss: 0.0316\n",
      "Iteration: 30050, Time: 21048.7253 s, training loss: 0.0283\n",
      "Iteration: 30100, Time: 21079.6422 s, training loss: 0.0295\n",
      "Iteration: 30150, Time: 21110.3480 s, training loss: 0.0312\n",
      "Iteration: 30200, Time: 21140.7591 s, training loss: 0.0315\n",
      "Iteration: 30250, Time: 21171.4348 s, training loss: 0.0254\n",
      "Iteration: 30300, Time: 21202.1197 s, training loss: 0.0300\n",
      "Iteration: 30350, Time: 21233.3209 s, training loss: 0.0315\n",
      "Iteration: 30400, Time: 21264.3245 s, training loss: 0.0337\n",
      "Iteration: 30450, Time: 21295.2314 s, training loss: 0.0254\n",
      "Iteration: 30500, Time: 21326.2820 s, training loss: 0.0256\n",
      "Iteration: 30550, Time: 21357.6480 s, training loss: 0.0300\n",
      "-----EPOCH7----- done.  Validation loss:  0.041392454895405814\n",
      "-----EPOCH7----- done.  Validation MRR:  0.8418669752248064\n",
      "-----EPOCH8-----\n",
      "Iteration: 30600, Time: 21402.3423 s, training loss: 0.0314\n",
      "Iteration: 30650, Time: 21433.4283 s, training loss: 0.0306\n",
      "Iteration: 30700, Time: 21464.5741 s, training loss: 0.0314\n",
      "Iteration: 30750, Time: 21495.5814 s, training loss: 0.0300\n",
      "Iteration: 30800, Time: 21526.5366 s, training loss: 0.0257\n",
      "Iteration: 30850, Time: 21557.7632 s, training loss: 0.0274\n",
      "Iteration: 30900, Time: 21588.8228 s, training loss: 0.0292\n",
      "Iteration: 30950, Time: 21619.9401 s, training loss: 0.0285\n",
      "Iteration: 31000, Time: 21651.2255 s, training loss: 0.0294\n",
      "Iteration: 31050, Time: 21682.3225 s, training loss: 0.0284\n",
      "Iteration: 31100, Time: 21713.7508 s, training loss: 0.0323\n",
      "Iteration: 31150, Time: 21744.7419 s, training loss: 0.0330\n",
      "Iteration: 31200, Time: 21776.2588 s, training loss: 0.0327\n",
      "Iteration: 31250, Time: 21807.8232 s, training loss: 0.0271\n",
      "Iteration: 31300, Time: 21838.3456 s, training loss: 0.0303\n",
      "Iteration: 31350, Time: 21868.8252 s, training loss: 0.0324\n",
      "-----EPOCH8----- done.  Validation loss:  0.04486622610308517\n",
      "-----EPOCH8----- done.  Validation MRR:  0.8424228851257406\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_discriminator7.pt\n",
      "-----EPOCH9-----\n",
      "Iteration: 31400, Time: 21918.3132 s, training loss: 0.0302\n",
      "Iteration: 31450, Time: 21949.2493 s, training loss: 0.0325\n",
      "Iteration: 31500, Time: 21980.4310 s, training loss: 0.0269\n",
      "Iteration: 31550, Time: 22011.3458 s, training loss: 0.0281\n",
      "Iteration: 31600, Time: 22041.9318 s, training loss: 0.0302\n",
      "Iteration: 31650, Time: 22072.5186 s, training loss: 0.0348\n",
      "Iteration: 31700, Time: 22103.5095 s, training loss: 0.0326\n",
      "Iteration: 31750, Time: 22134.5433 s, training loss: 0.0296\n",
      "Iteration: 31800, Time: 22165.2681 s, training loss: 0.0265\n",
      "Iteration: 31850, Time: 22196.4255 s, training loss: 0.0324\n",
      "Iteration: 31900, Time: 22227.4358 s, training loss: 0.0308\n",
      "Iteration: 31950, Time: 22258.5543 s, training loss: 0.0238\n",
      "Iteration: 32000, Time: 22289.8313 s, training loss: 0.0290\n",
      "Iteration: 32050, Time: 22320.8496 s, training loss: 0.0277\n",
      "Iteration: 32100, Time: 22351.8365 s, training loss: 0.0309\n",
      "Iteration: 32150, Time: 22383.0457 s, training loss: 0.0343\n",
      "Iteration: 32200, Time: 22414.0438 s, training loss: 0.0332\n",
      "-----EPOCH9----- done.  Validation loss:  0.04552771486879255\n",
      "-----EPOCH9----- done.  Validation MRR:  0.8418583616481445\n",
      "-----EPOCH10-----\n",
      "Iteration: 32250, Time: 22458.7703 s, training loss: 0.0305\n",
      "Iteration: 32300, Time: 22489.6548 s, training loss: 0.0310\n",
      "Iteration: 32350, Time: 22520.7228 s, training loss: 0.0341\n",
      "Iteration: 32400, Time: 22551.4620 s, training loss: 0.0310\n",
      "Iteration: 32450, Time: 22582.5672 s, training loss: 0.0262\n",
      "Iteration: 32500, Time: 22613.6282 s, training loss: 0.0317\n",
      "Iteration: 32550, Time: 22644.6668 s, training loss: 0.0262\n",
      "Iteration: 32600, Time: 22675.4922 s, training loss: 0.0281\n",
      "Iteration: 32650, Time: 22707.2767 s, training loss: 0.0291\n",
      "Iteration: 32700, Time: 22737.7311 s, training loss: 0.0263\n",
      "Iteration: 32750, Time: 22768.0545 s, training loss: 0.0303\n",
      "Iteration: 32800, Time: 22798.4196 s, training loss: 0.0321\n",
      "Iteration: 32850, Time: 22828.8618 s, training loss: 0.0300\n",
      "Iteration: 32900, Time: 22859.4608 s, training loss: 0.0326\n",
      "Iteration: 32950, Time: 22889.8356 s, training loss: 0.0251\n",
      "Iteration: 33000, Time: 22920.0459 s, training loss: 0.0289\n",
      "-----EPOCH10----- done.  Validation loss:  0.04375524566365549\n",
      "-----EPOCH10----- done.  Validation MRR:  0.8432512305059034\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_discriminator9.pt\n"
     ]
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 2e-7\n",
    "optimizer_discriminator.param_groups[0]['lr'] = 2e-7\n",
    "\n",
    "nb_epochs = 10\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    print('-----EPOCH{}-----'.format(i+1))\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        \n",
    "        graph_embeddings, text_embeddings = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        \n",
    "        for _ in range(discriminator_iteration):\n",
    "            \n",
    "            gp = gradient_penalty(discriminator, text_embeddings, graph_embeddings)\n",
    "            text_scores = discriminator(text_embeddings)\n",
    "            graph_scores = discriminator(graph_embeddings)\n",
    "\n",
    "            loss_discriminator = torch.mean(text_scores) - torch.mean(graph_scores) + lambda_gp*gp\n",
    "\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            loss_discriminator.backward(retain_graph=True)\n",
    "            optimizer_discriminator.step()            \n",
    "          \n",
    "        triplet_loss = hard_triplet_loss(graph_embeddings, text_embeddings)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += triplet_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "    model.eval()       \n",
    "    val_loss = 0   \n",
    "    graphs = []\n",
    "    texts = []     \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        graph_embeddings, text_embeddings = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = hard_triplet_loss(graph_embeddings, text_embeddings)  \n",
    "        val_loss += current_loss.item()\n",
    "        graphs.extend(graph_embeddings.tolist())\n",
    "        texts.extend(text_embeddings.tolist())\n",
    "\n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "    print('-----EPOCH'+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    similarity = cosine_similarity(texts, graphs)\n",
    "    y_true = np.eye(len(similarity))\n",
    "    score = label_ranking_average_precision_score(y_true, similarity)\n",
    "    print('-----EPOCH'+str(i+1)+'----- done.  Validation MRR: ', str(score) )\n",
    "    best_validation_mrr = max(best_validation_mrr, score)\n",
    "    if best_validation_mrr==score:\n",
    "        current_directory = os.getcwd()\n",
    "        files = os.listdir(current_directory)\n",
    "\n",
    "        for file in files:\n",
    "            if file.startswith('model'):\n",
    "                file_path = os.path.join(current_directory, file)\n",
    "                os.remove(file_path)\n",
    "                \n",
    "        print('validation loss improoved saving checkpoint...')\n",
    "        save_path = os.path.join('./', 'model'+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        save_path = os.path.join('./', 'model_discriminator'+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_discriminator.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<02:05, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8345661185417758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:27<01:49, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347720445959282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:40<01:34, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8345126234488979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:53<01:19, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8349754461625541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:07<01:06, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8352960560787419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:20<00:53, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8344885807105265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:34<00:40, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347273251025582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:47<00:26, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834865089993428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:00<00:13, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8359520621952138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:13<00:00, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834989511164502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for _ in tqdm(range(10)):\n",
    "    graphs, texts = [], []\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        \n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        \n",
    "        graph_embeddings, text_embeddings = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        graphs.extend(graph_embeddings.tolist())\n",
    "        texts.extend(text_embeddings.tolist())\n",
    "    similarity = cosine_similarity(texts, graphs)\n",
    "    y_true = np.eye(len(similarity))\n",
    "    scores = label_ranking_average_precision_score(y_true, similarity)\n",
    "    print(scores)\n",
    "    res.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349144857994126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res)/len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
