{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32807/448429717.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch.utils.data import DataLoader as TorchDataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "from alignment import AlignmentModel,Discriminator, gradient_penalty, CombinedModel\n",
    "# from moemodel import MOEModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "from losses import hard_triplet_loss, expo_triplet_loss, info_nce_loss, contrastive_weighted_loss, cosine_contrastive_weighted_loss, sigmoid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "sub_batch_size = 32\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer)\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------loading pretrained--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlignmentModel(\n",
       "  (text_encoder): TextEncoder(\n",
       "    (text_encoder): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (text_forward): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "    (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (graph_encoder): GraphEncoder(\n",
       "    (gnns): ModuleList(\n",
       "      (0-4): 5 x SuperGATConv(768, 768, heads=6, type=MX)\n",
       "    )\n",
       "    (projection): Linear(in_features=300, out_features=768, bias=True)\n",
       "    (graph_norms): ModuleList(\n",
       "      (0-4): 5 x LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (graph_forward): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlignmentModel(in_channels=300, out_channels=768, graph_attention_head=6, type = 'TransformerConv', n_layers = 5)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "loss = 0\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 100\n",
    "best_validation_loss = 1000000\n",
    "best_validation_mrr = 0\n",
    "\n",
    "sub_batch_size = 32\n",
    "nb_epochs = 30\n",
    "loss = 0\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(nb_epochs):\n",
    "    print('-----EPOCH{}-----'.format(e+1))\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids.to(device)\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask.to(device)\n",
    "        batch.pop('attention_mask')\n",
    "        batch.to(device)\n",
    "        \n",
    "        acc = 0\n",
    "        partial_loss = 0\n",
    "        \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(batch.ptr) -1 , sub_batch_size):\n",
    "    \n",
    "            f_index_i = min(i+ sub_batch_size, len(batch.ptr) - 1)\n",
    "    \n",
    "            sub_batch = batch.batch[batch.ptr[i]: batch.ptr[f_index_i]]\n",
    "            \n",
    "            sub_batch = sub_batch - sub_batch[0]\n",
    "            \n",
    "            sub_x = batch.x[batch.ptr[i]: batch.ptr[f_index_i]]\n",
    "            \n",
    "            max_index = batch.ptr[f_index_i]\n",
    "            \n",
    "            min_index = batch.ptr[i]\n",
    "            \n",
    "            sub_edge_index = torch.stack(\n",
    "                                            (\n",
    "                                                batch.edge_index[0][(min_index <= batch.edge_index[0]) & (batch.edge_index[0] < max_index)],\n",
    "                                                batch.edge_index[1][(min_index <= batch.edge_index[1]) & (batch.edge_index[1] < max_index)]\n",
    "                                            ),\n",
    "                                            dim=0\n",
    "                                        )\n",
    "            \n",
    "            sub_edge_index = sub_edge_index - min_index        \n",
    "            \n",
    "            # sub_graph_batch = Data(x = sub_x, edge_index = sub_edge_index, batch = sub_batch).to(device)\n",
    "    \n",
    "            graph_embeddings = model.forward_graph(sub_x, sub_edge_index, sub_batch)   \n",
    "    \n",
    "            cosine = torch.zeros(len(input_ids), graph_embeddings.shape[0]).to(device)\n",
    "        \n",
    "            for j in range(0, len(input_ids), sub_batch_size):\n",
    "                \n",
    "                # sub_input_ids = input_ids[j : j + sub_batch_size]\n",
    "                \n",
    "                # sub_attention_mask = attention_mask[j : j + sub_batch_size]\n",
    "    \n",
    "                text_embeddings = model.forward_text(input_ids[j : j + sub_batch_size], \n",
    "                                attention_mask[j : j + sub_batch_size])\n",
    "                \n",
    "                sub_cosine = pairwise_cosine_similarity(text_embeddings, graph_embeddings)\n",
    "    \n",
    "                cosine[j:j + text_embeddings.shape[0]] = sub_cosine\n",
    "                \n",
    "            current_loss = cosine_contrastive_weighted_loss(cosine,  pos = i, dim =0)\n",
    "            current_loss.backward()\n",
    "            acc += 1\n",
    "            partial_loss += current_loss.item()\n",
    "            \n",
    "    \n",
    "        for j in range(0, len(input_ids), sub_batch_size):\n",
    "                \n",
    "            sub_input_ids = input_ids[j : j + sub_batch_size]\n",
    "            \n",
    "            sub_attention_mask = attention_mask[j : j + sub_batch_size]\n",
    "    \n",
    "            text_embeddings = model.forward_text(sub_input_ids, \n",
    "                            sub_attention_mask)\n",
    "            \n",
    "            cosine = torch.zeros(text_embeddings.shape[0], len(batch.ptr) - 1).to(device)\n",
    "            \n",
    "            for i in range(0, len(batch.ptr) -1 , sub_batch_size):\n",
    "        \n",
    "                f_index_i = min(i+ sub_batch_size, len(batch.ptr) - 1)\n",
    "        \n",
    "                sub_batch = batch.batch[batch.ptr[i]: batch.ptr[f_index_i]]\n",
    "                \n",
    "                sub_batch = sub_batch - sub_batch[0]\n",
    "                \n",
    "                sub_x = batch.x[batch.ptr[i]: batch.ptr[f_index_i]]\n",
    "                \n",
    "                max_index = batch.ptr[f_index_i]\n",
    "                \n",
    "                min_index = batch.ptr[i]\n",
    "                \n",
    "                sub_edge_index = torch.stack(\n",
    "                                                (\n",
    "                                                    batch.edge_index[0][(min_index <= batch.edge_index[0]) & (batch.edge_index[0] < max_index)],\n",
    "                                                    batch.edge_index[1][(min_index <= batch.edge_index[1]) & (batch.edge_index[1] < max_index)]\n",
    "                                                ),\n",
    "                                                dim=0\n",
    "                                            )\n",
    "                \n",
    "                sub_edge_index = sub_edge_index - min_index        \n",
    "                \n",
    "                # sub_graph_batch = Data(x = sub_x, edge_index = sub_edge_index, batch = sub_batch).to(device)\n",
    "        \n",
    "                graph_embeddings = model.forward_graph(sub_x, sub_edge_index, sub_batch)       \n",
    "                \n",
    "                sub_cosine = pairwise_cosine_similarity(text_embeddings, graph_embeddings)\n",
    "    \n",
    "                cosine[:,i:i + graph_embeddings.shape[0]] = sub_cosine\n",
    "                \n",
    "            current_loss = cosine_contrastive_weighted_loss(cosine, pos = j, dim =1)\n",
    "            current_loss.backward()\n",
    "            acc += 1\n",
    "            partial_loss += current_loss.item()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad /= acc\n",
    "                    \n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        loss += partial_loss\n",
    "        partial_loss = 0\n",
    "            \n",
    "        count_iter += 1\n",
    "        \n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0\n",
    "            \n",
    "    model.eval()       \n",
    "    val_loss = 0   \n",
    "    graphs = []\n",
    "    texts = []     \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        graph_embeddings = model.forward_graph(x = batch.x.to(device), edge_index = batch.edge_index.to(device), batch = batch.batch.to(device)) \n",
    "        text_embeddings = model.forward_text(input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        graphs.extend(graph_embeddings.tolist())\n",
    "        texts.extend(text_embeddings.tolist())\n",
    "\n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "    print('-----EPOCH'+str(e+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    similarity = cosine_similarity(texts, graphs)\n",
    "    y_true = np.eye(len(similarity))\n",
    "    score = label_ranking_average_precision_score(y_true, similarity)\n",
    "    print('-----EPOCH'+str(e+1)+'----- done.  Validation MRR: ', str(score) )\n",
    "    best_validation_mrr = max(best_validation_mrr, score)\n",
    "    if best_validation_mrr==score:\n",
    "        current_directory = os.getcwd()\n",
    "        files = os.listdir(current_directory)\n",
    "\n",
    "        for file in files:\n",
    "            if file.startswith('model'):\n",
    "                file_path = os.path.join(current_directory, file)\n",
    "                os.remove(file_path)\n",
    "                \n",
    "        print('validation loss improoved saving checkpoint...')\n",
    "        save_path = os.path.join('./', 'model'+str(e)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': e,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'MRR' : score,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))\n",
    "\n",
    "    if e and e%15 == 0:\n",
    "        optimizer.param_groups[0]['lr'] /= 10\n",
    "    \n",
    "        \n",
    "                \n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
